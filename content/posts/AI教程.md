+++
title = 'AI教程'
subtitle = ""
date = 2024-05-24T10:16:53+08:00
draft = false
toc = true
series = ['AI']
+++

[toc]

<baidu.com>


## pytorch入门

**数据, 模型, 损失函数, 优化器**

### 数据

#### Tensor

张量

神经网络的基础单元

| tensor          | 概念                       | 特点                                                  |
| --------------- | -------------------------- | ----------------------------------------------------- |
| leaf tensor     | 直接创建的张量             | grad_fn 为 None, 在反向传播后会保留 .grad 属性        |
| non-leaf tensor | 通过操作其他张量生成的张量 | grad_fn 为 操作函数 , 在反向传播时不会保留 .grad 属性 |

forward: leaf -> root
backward: root -> leaf

```python
# 增删改查

# 升降纬度
torch.squeeze()
torch.unsqueeze()
torch.permute() # 指定纬度顺序
```

#### Dataset 和 DataLoader

DataLoader 提供灵活加载Dataset的设置

数据划分

| 数据集                   | 作用                                                         | 占比    |
| ------------------------ | ------------------------------------------------------------ | ------- |
| 训练集（Training Set）   | 模型通过训练集学习特征和模式                                 | 70%-80% |
| 验证集（Validation Set） | 用于调整超参数、选择模型结构和监控模型在未见过的数据上的表现 | 10%-15% |
| 测试集（Test Set）       | 用于评估模型的最终性能                                       | 10%-15% |


#### 批量处理

神经网络的损失函数通常是一个非凸函数,也就是说它可能存在多个局部最优解

批量处理的随机性有助于模型跳出局部最小值，找到全局最小值。
减少内存占用

鞍点

噪声较大

### 模型前置知识

#### 神经网络基础

神经网络是一种模仿人类大脑结构的机器学习模型.

通过模拟人脑的学习方式，具有强大的数据拟合和特征学习能力，是深度学习的重要支柱

##### 基本概念

| 概念       | 类比              | 功能                                         | 数学表示                                      | 常见类型/示例 |
| ---------- | ----------------- | -------------------------------------------- | --------------------------------------------- | ------------- |
| **神经元** | 生物神经元        | 信息处理的基本单元                           | output = activation(∑(weight * input) + bias) | -             |
|            | **权重** 连接强度 | 决定输入对输出的影响                         | w                                             | -             |
|            | **偏置** 阈值     | 调整神经元输出                               | b                                             | -             |
|            | **激活函数** 开关 | 引入非线性                                   | **Sigmoid, Tanh, ReLU, LeakyReLU, ELU**等     | -             |
| **层**     | 楼层              | 信息处理的层次                               | 输入层、隐藏层、输出层                        | -             |
|            | 输入层            | 接收外界数据，作为神经网络的输入             |                                               |               |
|            | **隐藏层**        | 对输入数据进行复杂的**非线性变换，提取特征** |                                               |               |
|            | 输出层            | 输出神经网络的预测结果                       |                                               |               |

###### 网络层 layer

对于 隐藏层的数量 和 各个隐藏层的节点数: 常见策略是逐层减少节点数

通常，我们选择2的若干次幂作为层的宽度。
因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。


| 层         | 名称           | 作用                                                                         | 概念                                                |
| ---------- | -------------- | ---------------------------------------------------------------------------- | --------------------------------------------------- |
| 全连接层   | nn.Linear      | 每个神经元与上一层的所有神经元相连。它用于提取特征和进行非线性变换。         | 它的每一个输入都通过矩阵-向量乘法得到它的每个输出。 |
| 激活层     | nn.ReLU        | 引入非线性变换                                                               |                                                     |
| 批归一化层 | nn.BatchNorm2d | 对每个批次的输入数据进行标准化，减小不同特征的偏差，帮助加速训练并稳定网络。 |                                                     |
| dropout层  | nn.Dropout     | 随机将一部分神经元的输出设为0                                                |                                                     |

| 层         | 名称           | 作用                                                                                                 |
| ---------- | -------------- | ---------------------------------------------------------------------------------------------------- |
| 卷基层     | nn.Conv2d      | 通过卷积核对输入数据进行卷积操作，提取局部特征                                                       |
| 池化层     | nn.MaxPool2d   | 用于下采样操作，可以减少特征图的尺寸，减少计算量，并防止过拟合。最常见的池化操作是最大池化和平均池化 |
| 循环层     | nn.RNN         | 保留历史信息                                                                                         |
| 自注意力层 | nn.Transformer | 捕捉输入数据中长距离依赖关系                                                                         |


### 模型 model

一个模型由1个或多个layer 组成.

#### sequential

```python
from torch import nn

model = nn.Sequential(
    nn.Conv2d(1,20,5),
    nn.ReLU(),
    nn.Conv2d(20,64,5),
    nn.ReLU()
)
```

#### 自定义复杂逻辑

```python
from torch import nn

class FCNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(FCNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out
```



#### 模型参数

```python
# 查看参数
model.state_dict()

for name, param in model.named_parameters():
    print(name, param.size())

```

模型参数初始化

```python
nn.init.zeros_
nn.init.constant_
nn.init.normal_
```


### 损失函数

作用: 计算模型准确度的工具, (预测结果与真实值之间的差异)

不同任务类型的需求：

- 分类问题通常使用交叉熵损失Cross-Entropy Loss）:

  softmax: (0,1), 和为1
  $\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$

- 回归问题常用 均方误差（Mean Squared Error, MSE, L2损失）或 平均绝对误差（Mean Absolute Error, MAE, L1损失）

- 推荐系统可能使用排序损失等


### 优化器

autograd 和 Computation Graph

Computation Graph是一种有向无环图（DAG），用于表示张量之间的数学运算关系. 记录了张量之间的操作和依赖关系，支持自动求导功能

多次计算梯度时, 需要设置 retain_graph=True


更新模型参数

### 常见操作步骤

#### 模型训练流程

| 概念         | 类比         | 功能                                                                     | 数学表示                                        | 常见类型/示例 |
| ------------ | ------------ | ------------------------------------------------------------------------ | ----------------------------------------------- | ------------- |
| **前向传播** | 信息流动     | 从输入到输出的计算过程                                                   | -                                               | -             |
| **损失函数** | 误差衡量     | 衡量预测值与真实值之间的差异, 训练的目标是**最小化损失函数**             | 均方误差(MSE), 交叉熵损失(Cross-Entropy Loss)等 | -             |
| **反向传播** | 计算梯度     | 应用链式法则进行求导的过程, 最终目的是计算出**损失函数对每层参数的梯度** | -                                               | -             |
| **优化器**   | 参数更新工具 | 根据梯度**更新参数**                                                     | **SGD, Adam, RMSprop, Adagrad**等               | -             |


前向传播,得出结果 -> 对比答案, 计算损失 -> 反向传播, 获得梯度 -> 使用optimizer, 更新模型参数 

```python
model = Net()
optimizer = optim.Adam(model.parameters())

for epoch in range(epochs):
    # 1. 前向传播
    output = model(data)
    loss = criterion(output, target)
    
    # 2. 清零梯度
    optimizer.zero_grad()
    
    # 3. 反向传播: 计算梯度
    loss.backward()  
    # output.backward(torch.ones_like(output))
    
    # 4. 优化器: 更新参数
    optimizer.step()
```

##### gpu训练

```python
net = Net()
if torch.cuda.is_available():
    net = net.cuda()

loss_fn = nn.CrossEntropyLoss()
if torch.cuda.is_available():
    loss_fn = loss_fn.cuda()


if torch.cuda.is_available():
    inputs = inputs.cuda()
    labels = labels.cuda()


torch.cuda.device_count()

```

#### 模型保存加载

```python
import torch
import torchvision

# method1
vgg16 = torchvision.models.vgg16(weights=True)
torch.save(vgg16, '/app/output/vgg16.pth')

model = torch.load('/app/output/vgg16.pth', weights_only=False)
print(model)

# method2
torch.save(net.state_dict(), 'mlp.params')

clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
```


## 模型细化



### 时间

在深度学习中，Temporal Resolution（时间分辨率） 指的是模型在时间维度上处理数据的精细程度，即模型在多大程度上能够捕捉和解析时间序列数据中的细节变化。​这在处理视频、音频、传感器数据等时间相关的数据时尤为重要。


### 模型性能优化

问答 -> 用户反馈
SFT

包 peft

合并模型

- 参数调优
- 性能评估

### 模型压缩


### 模型量化

将模型的参数（如权重和激活值）从高精度转换为低精度的过程

llama.cpp
转gguf

### 模型蒸馏

大模型指导小模型

优化效果很明显:

在NLP领域，知识蒸馏可以用于提升小模型的推理性能，同时减少模型的参数量，提高模型的运行速度。例如，DistilBERT与BERT相比减少了40%的参数，同时保留了BERT 97%的性能，但提高了60%的速度


### 思维链...

预训练: 通常是在大规模无标注数据上通过自监督学习得到的，它们具有对一般自然语言结构的良好理解能力

微调: 针对具体下游任务（如文本分类、问答系统、命名实体识别等），使用相对较小规模但**有标签的目标数据集**对该模型的**部分或全部参数**进行进一步的训练。



在 prompt 中添加“Let’s think step by step”或“请一步步推理”等指令，无需示例也能触发内生的分步生成。

大模型一旦超越某规模阈值，链式推理成功率呈指数级跃升


DeepSeek通过**思维链技术**将复杂问题**拆解**为一系列连贯的中间推理步骤

机器学习：特别是深度学习，使代理可以从大量数据中学习模式，并据此作出预测或决策。
强化学习：一种特殊的机器学习形式，其中代理通过尝试不同的行动并在每次尝试后接收奖励或惩罚来学习最优策略。
(1) 强化学习的作用
- 目标：通过GRPO算法，模型学习生成更详细、更合理的推理步骤（长CoT）以提高奖励（如答案正确性）。
- 机制：训练时生成多个候选答案，奖励信号不仅评估最终答案正误，还隐式鼓励逻辑连贯的推理路径（如通过格式奖励）。


奖励模型（Reward Model）: 使用奖励模型替代奖励函数,模拟人类偏好, 用来优化别的模型

对于复杂的数学问题、逻辑推理或者需要多步操作的任务而言，链式思考提示尤为有效

### 记忆功能

...

### MOE

路由器（Router） + n个专家

可扩展性：添加更多专家可扩展模型能力，无需重新训练整个网络。
灵活性：动态适配不同任务需求（如NLP、CV、语音识别）。


中电九天的LoRA-MoE专利通过任务嵌入矩阵和线性转换层生成专家权重，需联合微调门控网络和专家参数。

### 模型部署


ONNX（Open Neural Network Exchange）是一种深度学习模型的“通用格式”。

### MCP

通过 MCP，AI 模型可以安全地访问本地文件、数据库、API 等外部资源。

## 常见问题

### 欠拟合: 训练集误差大. 

处理方法:

- 使用复杂的模型
- 特征工程
- 修改optim

### 过拟合: 训练集误差小, 测试集误差大. 

处理方法:

- 增加数据
  - 数据增强
- 降低模型复杂度
  - dropout: 随机丢弃神经网络中的神经元，防止过拟合
  - 对损失函数, 正则化
- early stopping?



Dropout: `nn.Dropout` 通过随机丢弃神经元来强制模型学习鲁棒特征，主要适用于深度神经网络。

正则化:（尤其是L1和L2）通过在损失函数中添加惩罚项来限制模型复杂度，有效地防止模型过拟合，提高模型的泛化能力, 适用于广泛的机器学习模型。


梯度消失：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。

使用ReLU等激活函数

梯度爆炸：参数更新过大，破坏了模型的稳定收敛；

梯度裁剪

### 避免陷入局部极小值

- 参数初始化
- 更换好的优化器
  - 动量梯度下降


为了打破对称性，实践中常采用随机初始化的方法为每个神经元分配不同的初始权重。例如，Xavier/Glorot初始化、He初始化等方法都是基于不同的理论基础来随机初始化权重，以确保每个神经元从训练开始就具有一定的独特性，促进更有效的学习

## 其他名词

| 名词   | 概念                                                                     |
| ------ | ------------------------------------------------------------------------ |
| 鲁棒性 | 指的是模型对数据中的噪声、异常值或分布变化的抵抗能力                     |
| 对称性 | 指的是神经网络中不同神经元之间由于参数初始化相同而导致的相似或相同的行为 |
| 最大路径长度（Maximum Path Length）| 指的是在网络中，从序列的一个位置到另一个位置传递信息所需经过的最大步骤数。这一指标反映了**模型捕获长距离依赖关系的效率** |
| 词元（Token）| 模型可处理文本的最小单元, 包括 单词, 子词(词根root),字符等 |
| 词元化(Tokenization) | 把文本分解成 单词,字符的过程 |
| 正则化（Regularization）| 防止过拟合，提高模型泛化能力; 调整**损失函数**来控制**模型参数** |
| 激活值（Activation Value）| 是指每个神经元在接收到输入信号后，经过加权求和和激活函数处理后产生的输出值 |
| 归一化（Normalization）| 加快训练速度; 调整 输入数据或神经网络的激活值的**分布** |
| 原始输出（logits）| 未经过 Sigmoid 激活的输出 |
| predictions  | 输出经过 Sigmoid 激活函数 |






| 方法               | 归一化维度                     | 对批量大小敏感 | 适用场景                         | 备注说明                                                                 |
|--------------------|-------------------------------|----------------|----------------------------------|--------------------------------------------------------------------------|
| Batch Normalization (BN) | 对每个特征在整个批次上归一化       | 是             | 卷积神经网络（CNN）等大批量训练     | 加速训练，减小内部协变量偏移，但在小批量或RNN中效果不佳。 |
| Layer Normalization (LN) | 对每个样本的所有特征归一化         | 否             | 循环神经网络（RNN）、Transformer等 | 对序列建模有效，适用于小批量训练。 |
| Instance Normalization (IN) | 对每个样本的每个通道归一化       | 否             | 图像风格迁移、图像生成等           | 强调图像的风格特征，常用于生成任务。 |
| Group Normalization (GN) | 对每个样本的通道分组归一化         | 否             | 小批量训练的卷积神经网络（CNN）    | 在小批量训练中表现稳定，兼具BN和LN的优点。 |
| Weight Normalization (WN) | 对神经网络的权重向量归一化         | 否             | 各类神经网络                     | 加速训练，减少对初始化的敏感性。 |
| Spectral Normalization (SN) | 对权重矩阵的谱范数归一化         | 否             | 生成对抗网络（GAN）等             | 控制Lipschitz常数，提升训练稳定性。 |



## 番外

### 用摄像头监控员工

这种做法可能会带来一些严重的问题:

- 侵犯隐私：持续摄像监控会严重侵犯员工的隐私权。
- 破坏信任：这表明公司不信任员工，会极大损害员工与管理层之间的关系。
- 增加压力：被持续监视会给员工带来巨大的精神压力，可能导致工作效率下降和健康问题。
- 法律风险：在许多地方,这种做法可能违反劳动法和隐私法。
- 误判问题：简单的视频监控很难准确判断一个人是否在工作。思考、讨论等看似"不工作"的行为可能是工作过程的重要组成部分。
- 士气低落：这种管理方式会严重打击员工积极性,可能导致优秀员工离职。

