+++
title = 'AI教程'
subtitle = ""
date = 2024-05-24T10:16:53+08:00
draft = false
toc = true
series = ['AI']
+++

[toc]

<baidu.com>





## 模型细化



### 模型蒸馏

大模型指导小模型

优化效果很明显:

在NLP领域，知识蒸馏可以用于提升小模型的推理性能，同时减少模型的参数量，提高模型的运行速度。例如，DistilBERT与BERT相比减少了40%的参数，同时保留了BERT 97%的性能，但提高了60%的速度


### 思维链...

预训练: 通常是在大规模无标注数据上通过自监督学习得到的，它们具有对一般自然语言结构的良好理解能力

微调: 针对具体下游任务（如文本分类、问答系统、命名实体识别等），使用相对较小规模但**有标签的目标数据集**对该模型的**部分或全部参数**进行进一步的训练。



在 prompt 中添加“Let’s think step by step”或“请一步步推理”等指令，无需示例也能触发内生的分步生成。

大模型一旦超越某规模阈值，链式推理成功率呈指数级跃升


DeepSeek通过**思维链技术**将复杂问题**拆解**为一系列连贯的中间推理步骤

机器学习：特别是深度学习，使代理可以从大量数据中学习模式，并据此作出预测或决策。
强化学习：一种特殊的机器学习形式，其中代理通过尝试不同的行动并在每次尝试后接收奖励或惩罚来学习最优策略。
(1) 强化学习的作用
- 目标：通过GRPO算法，模型学习生成更详细、更合理的推理步骤（长CoT）以提高奖励（如答案正确性）。
- 机制：训练时生成多个候选答案，奖励信号不仅评估最终答案正误，还隐式鼓励逻辑连贯的推理路径（如通过格式奖励）。


奖励模型（Reward Model）: 使用奖励模型替代奖励函数,模拟人类偏好, 用来优化别的模型

对于复杂的数学问题、逻辑推理或者需要多步操作的任务而言，链式思考提示尤为有效

### 记忆功能

...

### MOE

路由器（Router） + n个专家

可扩展性：添加更多专家可扩展模型能力，无需重新训练整个网络。
灵活性：动态适配不同任务需求（如NLP、CV、语音识别）。


中电九天的LoRA-MoE专利通过任务嵌入矩阵和线性转换层生成专家权重，需联合微调门控网络和专家参数。


### MCP

通过 MCP，AI 模型可以安全地访问本地文件、数据库、API 等外部资源。

## 常见问题

### 欠拟合: 训练集误差大. 

处理方法:

- 使用复杂的模型
- 特征工程
- 修改optim

### 过拟合: 训练集误差小, 测试集误差大. 

处理方法:

- 增加数据
  - 数据增强
- 降低模型复杂度
  - dropout: 随机丢弃神经网络中的神经元，防止过拟合
  - 对损失函数, 正则化
- early stopping?



Dropout: `nn.Dropout` 通过随机丢弃神经元来强制模型学习鲁棒特征，主要适用于深度神经网络。

正则化:（尤其是L1和L2）通过在损失函数中添加惩罚项来限制模型复杂度，有效地防止模型过拟合，提高模型的泛化能力, 适用于广泛的机器学习模型。


梯度消失：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。

使用ReLU等激活函数

梯度爆炸：参数更新过大，破坏了模型的稳定收敛；

梯度裁剪

### 避免陷入局部极小值

- 参数初始化
- 更换好的优化器
  - 动量梯度下降


为了打破对称性，实践中常采用随机初始化的方法为每个神经元分配不同的初始权重。例如，Xavier/Glorot初始化、He初始化等方法都是基于不同的理论基础来随机初始化权重，以确保每个神经元从训练开始就具有一定的独特性，促进更有效的学习

## 其他名词

| 名词   | 概念                                                                     |
| ------ | ------------------------------------------------------------------------ |
| 鲁棒性 | 指的是模型对数据中的噪声、异常值或分布变化的抵抗能力                     |
| 对称性 | 指的是神经网络中不同神经元之间由于参数初始化相同而导致的相似或相同的行为 |
| 最大路径长度（Maximum Path Length）| 指的是在网络中，从序列的一个位置到另一个位置传递信息所需经过的最大步骤数。这一指标反映了**模型捕获长距离依赖关系的效率** |
| 词元（Token）| 模型可处理文本的最小单元, 包括 单词, 子词(词根root),字符等 |
| 词元化(Tokenization) | 把文本分解成 单词,字符的过程 |
| 正则化（Regularization）| 防止过拟合，提高模型泛化能力; 调整**损失函数**来控制**模型参数** |
| 激活值（Activation Value）| 是指每个神经元在接收到输入信号后，经过加权求和和激活函数处理后产生的输出值 |
| 归一化（Normalization）| 加快训练速度; 调整 输入数据或神经网络的激活值的**分布** |
| 原始输出（logits）| 未经过 Sigmoid 激活的输出 |
| predictions  | 输出经过 Sigmoid 激活函数 |






| 方法               | 归一化维度                     | 对批量大小敏感 | 适用场景                         | 备注说明                                                                 |
|--------------------|-------------------------------|----------------|----------------------------------|--------------------------------------------------------------------------|
| Batch Normalization (BN) | 对每个特征在整个批次上归一化       | 是             | 卷积神经网络（CNN）等大批量训练     | 加速训练，减小内部协变量偏移，但在小批量或RNN中效果不佳。 |
| Layer Normalization (LN) | 对每个样本的所有特征归一化         | 否             | 循环神经网络（RNN）、Transformer等 | 对序列建模有效，适用于小批量训练。 |
| Instance Normalization (IN) | 对每个样本的每个通道归一化       | 否             | 图像风格迁移、图像生成等           | 强调图像的风格特征，常用于生成任务。 |
| Group Normalization (GN) | 对每个样本的通道分组归一化         | 否             | 小批量训练的卷积神经网络（CNN）    | 在小批量训练中表现稳定，兼具BN和LN的优点。 |
| Weight Normalization (WN) | 对神经网络的权重向量归一化         | 否             | 各类神经网络                     | 加速训练，减少对初始化的敏感性。 |
| Spectral Normalization (SN) | 对权重矩阵的谱范数归一化         | 否             | 生成对抗网络（GAN）等             | 控制Lipschitz常数，提升训练稳定性。 |



## 番外

### 用摄像头监控员工

这种做法可能会带来一些严重的问题:

- 侵犯隐私：持续摄像监控会严重侵犯员工的隐私权。
- 破坏信任：这表明公司不信任员工，会极大损害员工与管理层之间的关系。
- 增加压力：被持续监视会给员工带来巨大的精神压力，可能导致工作效率下降和健康问题。
- 法律风险：在许多地方,这种做法可能违反劳动法和隐私法。
- 误判问题：简单的视频监控很难准确判断一个人是否在工作。思考、讨论等看似"不工作"的行为可能是工作过程的重要组成部分。
- 士气低落：这种管理方式会严重打击员工积极性,可能导致优秀员工离职。

