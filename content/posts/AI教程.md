+++
title = 'AI教程'
subtitle = ""
date = 2024-05-24T10:16:53+08:00
draft = true
toc = true
tags = []
+++



## 用摄像头监控员工

这种做法可能会带来一些严重的问题:

- 侵犯隐私：持续摄像监控会严重侵犯员工的隐私权。
- 破坏信任：这表明公司不信任员工，会极大损害员工与管理层之间的关系。
- 增加压力：被持续监视会给员工带来巨大的精神压力，可能导致工作效率下降和健康问题。
- 法律风险：在许多地方,这种做法可能违反劳动法和隐私法。
- 误判问题：简单的视频监控很难准确判断一个人是否在工作。思考、讨论等看似"不工作"的行为可能是工作过程的重要组成部分。
- 士气低落：这种管理方式会严重打击员工积极性,可能导致优秀员工离职。

## 大模型

预训练->大模型

## 神经网络架构

| 神经网络架构       | 定义与特点       | 应用场景    |
|--------------------|------------------|-------------|
| **CNN（Convolutional Neural Network）** | 适合处理图像数据，通过卷积层提取局部特征，池化层降采样，全连接层用于分类或回归 |  图像分类（如手写数字识别）                |
|                         |                                                                                |  物体检测（如 YOLO）                      |
|                         |                                                                                |  图像分割（如 U-Net）                     |
| **RNN（Recurrent Neural Network）** | 适合处理序列数据，能记住历史信息，处理时间序列；梯度消失/爆炸问题需注意       | - 自然语言处理（如机器翻译、文本生成）       |
|                    | LSTM、GRU 作为改进版本，分别引入门机制，解决梯度问题       | - 时间序列预测（如股票预测）               |
|                    |                                                            | - 手写体识别                               |
| **Transformer**    | 基于注意力机制的架构，可并行处理序列中的各个位置           | - 自然语言处理（如 BERT、GPT）             |
|                    |                                                            | - 图像处理（如 Vision Transformer）        |
| **GAN（生成对抗网络）** | 由生成器和判别器对抗组成，用于生成逼真的数据          | - 图像生成（如高分辨率照片生成）           |
|                    |                                                            | - 风格迁移（图像风格转换）                 |
|                    |                                                            | - 数据增强（生成合成数据）                 |
| **Autoencoder（自编码器）** | 无监督学习算法，用于数据降维和特征提取，包括编码器和解码器         | - 数据降维（高维数据可视化）               |
|                    |                                                            | - 图像去噪（恢复清晰图像）                 |
|                    |                                                            | - 异常检测（检测异常样本）                 |
| **MLP（多层感知机）**  | 最基本的前馈神经网络，由多个全连接层组成，适合处理结构化数据             | - 分类和回归任务                            |
|                    |                                                            | - 数据特征学习（深度学习基础结构）         |


## 数学相关

### 微积分

#### 导数的定义

导数用于描述函数在某一点的变化率或斜率。对于一个单变量函数 \( f(x) \)，导数表示为：
\[
f'(x) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}
\]
这个表达式表示当 \( \Delta x \) 无限趋近于零时，函数值的变化率，即函数的瞬时变化速率。

导数可以理解为函数在某一点的切线斜率，用来表示函数的变化趋势。导数的值可以是正的、负的或零，分别对应函数的上升、下降或平稳状态。

#### 导数的常见规则

- **常数的导数**：\( f(x) = c \)，其中 \( c \) 是常数，则 \( f'(x) = 0 \)。
- **幂函数的导数**：\( f(x) = x^n \)，则 \( f'(x) = nx^{n-1} \)。
- **和的导数**：\( f(x) = g(x) + h(x) \)，则 \( f'(x) = g'(x) + h'(x) \)。
- **乘积的导数**：\( f(x) = g(x) \cdot h(x) \)，则 \( f'(x) = g'(x) \cdot h(x) + g(x) \cdot h'(x) \)。
- **链式法则**：如果 \( f(x) = g(h(x)) \)，则 \( f'(x) = g'(h(x)) \cdot h'(x) \)。

#### 偏导数的定义

偏导数表示一个多元函数对某个自变量的变化率，其余自变量保持不变。假设有一个多元函数 \( f(x, y) \)，则偏导数表示为：
\[
\frac{\partial f}{\partial x} \quad \text{和} \quad \frac{\partial f}{\partial y}
\]
分别表示在 \( y \) 固定时，\( f \) 随 \( x \) 的变化率，以及在 \( x \) 固定时，\( f \) 随 \( y \) 的变化率。

#### 偏导数的例子

假设有一个函数：
\[
f(x, y) = x^2 + 3xy + y^2
\]

- 计算 \( f \) 对 \( x \) 的偏导数（保持 \( y \) 不变）：
  \[
  \frac{\partial f}{\partial x} = 2x + 3y
  \]

- 计算 \( f \) 对 \( y \) 的偏导数（保持 \( x \) 不变）：
  \[
  \frac{\partial f}{\partial y} = 3x + 2y
  \]

在这些计算中，我们分别只对 \( x \) 或 \( y \) 进行求导，将另一个变量看作常数。

#### 梯度的定义

梯度是由所有偏导数组成的一个向量，用来表示多元函数在某一点处的变化方向和速率。对于一个多元函数 \( f(x, y) \)，梯度用符号 \( \nabla f \) 表示，定义为：
\[
\nabla f = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)
\]
梯度向量的方向是函数值增加最快的方向，梯度的大小（模）表示函数沿着该方向的变化速率。

#### 小结

| 名词 | 概念 |
| --- | --- |
| 导数 | 描述单变量函数（即一个自变量的函数）的变化率 |
| 偏导数 | 描述多变量函数（即多个自变量的函数）的变化率, 函数在某一个方向上的变化率 |
| 梯度 Gradient | 偏导数的集合, 梯度向量的方向是函数值增加最快的方向，梯度的大小（模）表示函数沿着该方向的变化速率 |

它们都是表示函数值对自变量的敏感程度


### 线性代数

#### 数据类型

TODO:

| 名词 | 应用场景 | 纬度 |
| --- | --- | --- |
| 标量 | 点 |  |
| 向量 | 线 | 1 |
| 矩阵 | 面 | 2 |
| 张量 | 体 | 多 |

#### 基本运算

#### 点积 Dot Product

点积的计算例子:

例如，给定向量 \( \mathbf{a} = [1, 2, 3] \) 和 \( \mathbf{b} = [4, 5, 6] \)，它们的点积为：
\[
1 \times 4 + 2 \times 5 + 3 \times 6 = 4 + 10 + 18 = 32
\]

```python
sum(X*Y)
```

#### 元素乘法

```python
# 独立事件的概率相乘
p1 = np.array([0.3, 0.5, 0.8])  # 事件A的概率
p2 = np.array([0.4, 0.6, 0.7])  # 事件B的概率
joint_p = p1 * p2  # 联合概率
```

#### 矩阵乘法

如果 A, B是两个矩阵, A行和B列之间进行点积操作, 得到矩阵乘法的结果

这个规则保证了所有元素的运算顺序与维度一致，使得结果矩阵代表了两个线性变换的组合效果。

线性变换和向量空间的运算

```python
import numpy as np

# 创建两个矩阵
A = np.array([[1, 2, 3],
              [4, 5, 6]])

B = np.array([[7, 8],
              [9, 10],
              [11, 12]])

# 使用矩阵乘法运算符
C = A @ B

# 结果是
# [[58, 64],
#  [139,154]]

"""
计算原理:
C[0,0] = 1×7 + 2×9 + 3×11 = 58
C[0,1] = 1×8 + 2×10 + 3×12 = 64
C[1,0] = 4×7 + 5×9 + 6×11 = 139
C[1,1] = 4×8 + 5×10 + 6×12 = 154
"""

```
$$
\text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}}
$$

$$
\text{out}_{i} = \sqrt{\text{input}_{i}}
$$

#### 小结

| 名词 | 应用场景 |
| --- | --- |
| 元素乘法 | 特征遮掩, 图像处理 |
| 矩阵乘法 | 图像处理, 线性变换, 特征转换 |

#### norm

范数一种度量，通常用来衡量向量或矩阵的“大小”或“长度”
分为 l1, l2, 最大norm

l2最常用

## Tensor

张量

神经网络的基础单元

| tensor | 概念 | 特点 |
|---|---|---|
| leaf tensor | 直接创建的张量 | 在反向传播时会保留 .grad 属性 |
| non-leaf tensor | 通过操作其他张量生成的张量 | 在反向传播时不会保留 .grad 属性 |




## 模型训练

### 基本概念

#### 损失函数

对于一组数据，我们计算每个样本的预测值（w*x+b）和真实值（y）之间的差的平方，然后求和，得到的就是模型的总损失。

$$
\text{loss} = \sum_i \left( w \cdot x_i + b - y_i \right)^2
$$

训练的目标是最小化损失函数

#### 反向传播算法 

应用链式法则进行求导的过程, 最终目的是计算出损失函数对每层参数的梯度 

#### optimizer 优化器

梯度下降算法

#### 激活函数

为模型增加非线性

### 模型训练流程

前向传播 -> 得出结果 -> 反向传播 -> 获得梯度 -> 使用optimizer, 更新模型参数 

```python
model = Net()
optimizer = optim.Adam(model.parameters())

for epoch in range(epochs):
    # 1. 前向传播
    output = model(data)
    loss = criterion(output, target)
    
    # 2. 清零梯度
    optimizer.zero_grad()
    
    # 3. 反向传播: 计算梯度
    loss.backward()  
    
    # 4. 优化器: 更新参数
    optimizer.step()
```

## 模型微调

问答 -> 用户反馈
SFT

包 peft

合并模型


## 模型量化

是将模型的参数（如权重和激活值）从高精度（通常是32位浮点数）转换为低精度表示（如8位整数或更低位数）的过程

llama.cpp
转gguf


